# Text corpus
This is a list of text corpora available to free of charge

1st row is description, 2nd is URL, and last is license or terms of use (if any)

## Japanese

### Annotated

#### 京都大学テキストコーパス (京大コーパス)
- 毎日新聞の記事に各種言語情報を人手で付与したテキストコーパスです。95年1月1日から17日までの全記事、約2万文、1月から12月までの社説記事、約2万文、計約4万文に対して、形態素・構文情報を付与しています。これらの情報は、形態素解析システムJUMAN、構文解析システムKNPで自動解析を行い、その結果を人手で修正したものです。さらに、このうちの5,000文に対しては、格関係、照応・省略関係、共参照の情報を付与しています。
- http://nlp.ist.i.kyoto-u.ac.jp/index.php?%E4%BA%AC%E9%83%BD%E5%A4%A7%E5%AD%A6%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9

#### NAISTテキストコーパス
- 京都テキストコーパスで利用されている毎日新聞95年1月1日から17日まで の全記事、約2万文、1月から12月までの社説記事、約2万文、計約4万文に対して，述語と表層格（ガ格，ヲ格，ニ格）の関係，事態性名詞と表層格（ガ格， ヲ格，ニ格）の関係，事態性名詞の名詞クラス，名詞句間の共参照関係，指示連体詞・代名詞の照応関係の情報を付与したコーパスです．
- https://sites.google.com/site/naisttextcorpus/
- 修正 BSD ライセンス

#### 解析済みブログコーパス (KNBC)
- 4テーマ（京都観光、携帯電話、スポーツ、グルメ）、249記事、4,186文の解析済みブログコーパス。形態素、構文、格・省略・照応、評判情報がアノテーションされている。
- http://nlp.ist.i.kyoto-u.ac.jp/kuntt

#### 京都大学ウェブ文書リードコーパス
- 本コーパスは、さまざまなウェブ文書のリード(冒頭)3文に各種言語情報を人手で付与したテキストコーパスです。ウェブ文書のリード3文を収集することによって、ニュース記事、百科事典記事、ブログ、商用ページなど多様なジャンル、文体の文書を含んでいます。コーパスの規模は約5,000文書です。
- http://nlp.ist.i.kyoto-u.ac.jp/index.php?KWDLC

#### 場所参照表現タグ付きコーパス
- 本コーパスは，Twitterからランダムにサンプリングしたテキストに現れる，「特定の場所を著者が想定している」と判断できる表現に対して，実際にどのエンティティを指しているかを人手で判断しエンティティ情報を付与したコーパスです．GeoNLPなどのジオパーズシステム，エンティティリンキングシステム等の開発や評価に利用することを想定して構築されました．
- http://www.cl.ecei.tohoku.ac.jp/~matsuda/LRE_corpus/
- creative commons

#### 近代女性雑誌コーパス
- 明治後期～大正期の女性雑誌3種から40冊を抽出した全文コーパス / 本文テキストに、XMLを用いて文書構造・文字・表記に関する情報をタグ付け / 対象雑誌 ：　『女学雑誌』（女学雑誌社）　1894（明治27）年・1895（明治28）年　31冊『女学世界』（博文館）　1909（明治42）年　6冊『婦人倶楽部』（講談社）　1925（大正14）年　3冊 / 総文字数 ： 約210万字 / 著者数 ： 約480人
- http://pj.ninjal.ac.jp/corpus_center/cmj/woman-mag/
- CC BY-NC-ND 3.0

#### 日本語自然会話書き起こしコーパス（旧名大会話コーパス）
- 約100時間分の雑談を文字化したコーパスです。 
- https://nknet.ninjal.ac.jp/nuc/templates/nuc.html

### Raw text
#### livedoor ニュースコーパス (livedoor news corpus)
- 本コーパスは、NHN Japan株式会社が運営する「livedoor ニュース」のうち、下記のクリエイティブ・コモンズライセンスが適用されるニュース記事を収集し、可能な限りHTMLタグを取り除いて作成したものです。
- http://www.rondhuit.com/download.html
- creative commons

#### 青空文庫
- 青空文庫は、誰にでもアクセスできる自由な電子本を、図書館のようにインターネット上に集めようとする活動です。著作権の消滅した作品と、「自由に読んでもらってかまわない」とされたものを、テキストとＸＨＴＭＬ（一部はＨＴＭＬ）形式に電子化した上で揃えています。
- http://www.aozora.gr.jp

#### プロジェクト杉田玄白
- プロジェクト杉田玄白というのは、いろんな文章を勝手に翻訳して公開しちゃうプロジェクトなのだ。プロジェクトグーテンベルグや、青空文庫の翻訳版だと思って欲しい。
- http://www.genpaku.org
- [丸ごとコピーしたいときも、文章自体を変えないで（変える場合にも変えたところを明示すればオッケー）、もとのURLと版権と、転載自由だってのさえ明記すればなーんも文句はつけないので、これも黙ってやってくれ。](http://cruel.org/linkpolicy.html)

#### 日本語ウェブコーパス 2010
- 本コーパスの HTML アーカイブは，ipadic-2.7.0 の見出し語をシードとして，Yahoo! Web API による検索結果に含まれるウェブページを収集したものです．テキストの抽出においては，文字コードを UTF-8 に統一した後，いくつかの記号をデリミタとして文への分割をおこない，さらに文を構成する文字の種類や数によるフィルタリングを施しています．
- http://s-yata.jp/corpus/nwc2010/

#### TomoriNao
- Charlotteのアニメにおける友利奈緒のセリフをまとめました
- https://github.com/tomorinao/corpus

#### 「日本の消滅危機言語・方言」データ
- 日本の消滅危機言語・方言の音声データを紹介するページです。国立国語研究所では、共同研究プロジェクト「消滅危機方言の調査・保存のための総合的研究」(2010年度～2015年度)，「日本の消滅危機言語・方言の記録とドキュメンテーションの作成」(2016～2011年度)を実施しています。
- http://kikigengo.ninjal.ac.jp

#### Microsoft IME Corpus
- it provides a test data set for the task of Japanese character conversion for text input.
- https://www.microsoft.com/en-us/download/details.aspx?id=52316

#### ja.text8
- ja.text8 is a small (100MB) text corpus from the web (japanese wikipedia).
- https://github.com/Hironsan/ja.text8

### N-gram
#### NINJAL Web Japanese Corpus
- 『国語研日本語ウェブコーパス』はウェブを母集団として100 億語規模を目標として構築した日本語コーパスです。 ウェブ（WWW）上の日本語テキストを利用して100億語を超える規模の現代日本語コーパスを構築することによって、稀言語現象の言語学的、心理学的および情報処理的視点からの究明の可能性を開くことを目的としています。 具体的な応用として、言語研究のための用例収集、日本語使用実態の定量的な把握などを想定しています。
- http://nwjc-data.ninjal.ac.jp/

#### 言語モデル配布ページ
- ウェブデータ、現代日本語書き言葉コーパスから作成した音声認識用と仮名漢字変換用の N-gram を配布しています。
- http://www.ar.media.kyoto-u.ac.jp/member/gologo/lm.html

#### Baidu ブログ・掲示板時間軸コーパス
- Baiduのクロールしたウェブデータから、掲示板の書き込みおよびブログの本文を、書き込まれた正確な時間とともに抽出し時系列に並べたデータを元にして作成したコーパスです。2000年1月～2010年7月の期間に対して、各1ヶ月ごとにスライスし、Nグラム（1グラム～3グラム）の統計を計算しています。
- https://web.archive.org/web/*/http://www.baidu.jp/corpus/
- https://web.archive.org/web/20101130153412/http://www.baidu.jp/corpus/Baidu_Terms_and_condition_Corpus.pdf

#### Baidu 絵文字入りモバイルウェブコーパス
- 一般に公開されているAPIやプライベートなAPI、あるいは以下のBaidu絵文字入りモバイルウェブコーパス（Baiduがウェブから抽出した絵文字を含む日本語データから作成したNグラムデータ）です。
- https://web.archive.org/web/*/http://www.baidu.jp/corpus/
- https://web.archive.org/web/20101130153412/http://www.baidu.jp/corpus/Baidu_Terms_and_condition_Corpus.pdf

### etc
#### 言語処理学会論文誌LaTeXコーパス
- 会誌「自然言語処理」に掲載された論文のLaTeXのソースファイル
- http://www.anlp.jp/resource/journal_latex/

#### cannadic改
- cannadic を拡充した"かな漢字変換辞書"です。
- https://osdn.jp/projects/alt-cannadic/

#### NAIST-JENE
- Wikipedia (2005年10月29日版)に対する NYU 関根さんの拡張固有表現階層(v.6.1.4)を付与したもの。
- https://github.com/masayu-a/NAIST-JENE
- 研究利用の場合は自由

#### 日本語単語類似度データセット (JapaneseWordSimilarityDataset)
- 低頻度語を含む日本語に関する類似度データセットを構築しました。対象とした品詞は、動詞 (verb)・形容詞 (adj)・名詞 (noun)・副詞 (adv)になっています。
- https://github.com/tmu-nlp/JapaneseWordSimilarityDataset

#### SlothLib ストップワードリスト
- http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt

#### SNOW D18:日本語感情表現辞書
- 約2,000表現を収録し、各表現に対して我々が独自に定義した48分類の感情を付与しています。
- http://www.jnlp.org/SNOW/D18

#### 📙UNICODE絵文字の日本語読み/キーワード/分類辞書📙
- emoji_jaは、Unicodeに登録されている絵文字に対して、日本語の読みやキーワード、分類を付与したデータセットです。Unicodeで定められている名称やアノテーションを元に構築しています。
- https://yag-ays.github.io/project/emoji-ja/
- MIT LICENSE

#### Twitter日本語評判分析データセット
- ツイートの評判情報をクラウドソーシングにより分析し，分析結果を公開しています．携帯電話などのツイートを中心に，534,962件のツイートがの分析が行われています．このツイート量は，他のデータセットと比較しても多いです．作成者の知る限り最も規模が大きく，種類数の大きなデータセットです．最低 4 名以上の作業者により評価を行い，多数決を行った結果です．だいたい5名以上の作業者により評価を行っています．
- http://bigdata.naist.jp/~ysuzuki/data/twitter/
- CC-BY-ND 4.0

## English

### Annotated

#### The Open American National Corpus
- The Open American National Corpus (OANC) is a massive electronic collection of American English, including texts of all genres and transcripts of spoken data produced from 1990 onward. All data and annotations are fully open and unrestricted for any use.
- http://www.anc.org/

#### Cornell Movie--Dialogs Corpus
- This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts
- http://www.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html

#### TYPO CORPUS
- This corpus is a collection of typos in tweets. The corpus consists of pairs of a typo and its original form (such as gogle:google). The typos dealt with this research are classified into four types
- http://luululu.com/tweet/

#### 30M Factoid Question-Answer Corpus
- The 30M Factoid Question-Answer Corpus consists of 30M natural language questions in English and their corresponding facts in the knowledge base Freebase.
- http://agarciaduran.org/index.php/2016/02/20/data-set/

#### Wikipedia Discussion Corpora
- 英語版Wikipediaでの議論をコーパスにしたもの。
- https://www.ukp.tu-darmstadt.de/data/discourse-analysis/wikipedia-discussion-corpora/

#### HappyDB
- HappyDB is a corpus of 100,000+ crowd-sourced happy moments.
- https://github.com/rit-public/HappyDB

### Raw

#### The 20 Newsgroups data set
- The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.
- http://qwone.com/~jason/20Newsgroups

#### The University of Oxford Text Archive (OTA)
- The University of Oxford Text Archive develops, collects, catalogues and preserves electronic literary and linguistic resources for use in Higher Education, in research, teaching and learning. The OTA also gives advice on the creation and use of these resources, and is involved in the development of standards and infrastructure for electronic language resources.
- https://ota.ox.ac.uk/

## Parallel corpus
#### Wikipedia日英京都関連文書対訳コーパス
- https://alaginrc.nict.go.jp/WikiCorpus/

#### Tanaka Corpus
- the Tanaka Corpus of parallel Japanese-English sentences
- http://www.edrdg.org/wiki/index.php/Tanaka_Corpus

#### 日西対照オノマトペコーパス
- 日本語の擬声語・擬態語（オノマトペ）とそれに対応するスペイン語の翻訳をコーパスの形にしたものです
- http://www.tufs.ac.jp/st/club/sevilla/

#### small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods
- This directory includes a small parallel corpus for English-Japanese translation task. These data are extracted from TANAKA Corpus by filtering sentence length 4 to 16 words.
- https://github.com/odashi/small_parallel_enja

#### OPUS
- OPUS is a growing collection of translated texts from the web. In the OPUS project we try to convert and align free online data, to add linguistic annotation, and to provide the community with a publicly available parallel corpus. 
- http://opus.lingfil.uu.se/

#### SNOW T15:やさしい日本語コーパス
- ５万文をやさしい日本語（平易な日本語語彙）に書き換えた対訳コーパスです。テキストは機械翻訳用の日英対訳コーパスである small_parallel_enja: 50k En/Ja Parallel Corpus forTesting SMT Methods を使用し、このコーパスの各日本語文にやさしい日本語を付与する形でコーパスを作成しました。
- http://www.jnlp.org/SNOW/T15
- CC BY 4.0
